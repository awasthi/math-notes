%
% Notes on Mathematics
% John Peloquin
%
% Algebra
% Vector Spaces
% Operators on Inner Product Spaces
%
\section{Operators on Inner Product Spaces}
Let \(V\)~be an inner product space over~\(\F\).
\subsection*{Definitions}
\begin{defn}
An operator \(T\in\Hom(V)\) is \emph{normal} if \(T^*T=TT^*\).
\end{defn}

\begin{defn}
An operator \(T\in\Hom(V)\) is \emph{self-adjoint} if \(T=T^*\).
\end{defn}

\begin{defn}
An operator \(T\in\Hom(V)\) is \emph{positive} if \(T\)~is self-adjoint and \(\innerprod{Tv}{v}\ge0\) for all \(v\in V\).
\end{defn}

\begin{defn}
An operator \(T\in\Hom(V)\) is an \emph{isometry} if \(\norm{Tv}=\norm{v}\) for all \(v\in V\).
\end{defn}

\begin{defn}
The \emph{singular values} of an operator \(T\in\Hom(V)\) are the eigenvalues of~\(\sqrt{T^*T}\).
\end{defn}

\begin{defn}
A square matrix~\(M\) is \emph{block diagonal} if
\[M=\left[\begin{matrix}
M_1&&0\\
&\ddots&\\
0&&M_n
\end{matrix}\right]\]
where each~\(M_i\) is a square block.
\end{defn}

\subsection*{Theorems}
\begin{rmk}
It is useful to think intuitively of operators on complex inner product spaces as generalizations of complex numbers, which are just operators on the complex plane through multiplication. Normal operators correspond to complex numbers; adjoints, to conjugates; self-adjoint operators, to real numbers; positive operators, to nonnegative numbers; and isometries, to the unit circle. In addition, the polar decomposition is just polar form for operators.
\end{rmk}
\begin{thm}[Definiteness]
Let \(T\in\Hom(V)\).
\begin{enumerate}[itemsep=0pt]
\item[(a)] If \(V\)~is complex and \(\innerprod{Tv}{v}=0\) for all \(v\in V\), then \(T=0\).
\item[(b)] If \(T\)~is self-adjoint and \(\innerprod{Tv}{v}=0\) for all \(v\in V\), then \(T=0\).
\end{enumerate}
\end{thm}
\begin{proof}[Proof idea]
By expressing~\(\innerprod{Tu}{w}\) for arbitrary \(u,w\in V\) as a linear combination of inner products of the form~\(\innerprod{Tv}{v}\), then taking \(w=Tu\).
\end{proof}
\begin{cor}
If \(V\)~is complex, \(T\)~is self-adjoint iff \(\innerprod{Tv}{v}\in\R\) for all \(v\in V\).
\end{cor}
\begin{proof}[Proof idea]
By translating between equality and nullity. For \(v\in V\),
\begin{align*}
\innerprod{Tv}{v}\in\R&\iff\innerprod{Tv}{v}=\conj{\innerprod{Tv}{v}}\\
	&\iff\innerprod{Tv}{v}=\innerprod{v}{Tv}\\
	&\iff\innerprod{Tv}{v}=\innerprod{T^*v}{v}\\
	&\iff\innerprod{(T-T^*)v}{v}=0\qedhere
\end{align*}
\end{proof}

\begin{thm}
Eigenvalues of self-adjoint operators are real.
\end{thm}
\begin{proof}[Proof idea]
Let \(T\in\Hom(V)\), \(\lambda\)~be an eigenvalue of~\(T\), and be a corresponding nonzero eigenvector~\(v\). Then
\[\lambda\innerprod{v}{v}=\innerprod{\lambda v}{v}=\innerprod{Tv}{v}=\innerprod{v}{Tv}=\innerprod{v}{\lambda v}=\conj{\lambda}\innerprod{v}{v}\]
Therefore \(\lambda=\conj{\lambda}\).
\end{proof}

\begin{thm}[Norm characterization of normality]
If \(T\in\Hom(V)\), then \(T\)~is normal iff \(\norm{Tv}=\norm{T^*v}\) for all \(v\in V\).
\end{thm}
\begin{proof}[Proof idea]
By translating between equality and nullity. For \(v\in V\),
\begin{align*}
\norm{Tv}=\norm{\textstyle T^*v}&\iff\innerprod{Tv}{Tv}=\innerprod{T^*v}{T^*v}\\
	&\iff\innerprod{T^*Tv}{v}=\innerprod{TT^*v}{v}\\
	&\iff\innerprod{(TT^*-T^*T)v}{v}=0
\end{align*}
The result follows by noting that \(TT^*-T^*T\) is self-adjoint.
\end{proof}

\begin{thm}[Spectral theorems]
Let \(V\)~be finite-dimensional and \(T\in\Hom(V)\).
\begin{enumerate}[itemsep=0pt]
\item[(a)] If \(V\)~is complex, then \(V\)~has an orthonormal basis consisting of eigenvectors of~\(T\) iff \(T\)~is normal.
\item[(b)] If \(V\)~is real, then \(V\)~has an orthonormal basis consisting of eigenvectors of~\(T\) iff \(T\)~is self-adjoint.
\end{enumerate}
\end{thm}
\begin{proof}[Proof idea]
For~(a), the forward direction is immediate by computing matrices. For the reverse direction, choose an orthonormal basis with respect to which \(\mat(T)\)~is upper triangular, then use the relationship between \(\mat(T)\)~and~\(\mat(T^*)\) and the norm characterization of normality to argue that \(\mat(T)\)~must be diagonal, and hence the basis vectors must be eigenvectors of~\(T\).

For~(b), again the forward direction is immediate. For the reverse direction, first argue that \(T\)~must have an eigenvalue by factoring an appropriate real polynomial operator in~\(T\)\footnote{See the proof that an operator on a real vector space has an invariant subspace of dimension \(1\)~or~\(2\).} and noting that any irreducible quadratic factors are injective since \(T\)~is self-adjoint. Then proceed by induction on~\(\dim V\), using the linear subspace generated by an eigenvector in an orthogonal decomposition in the induction step.
\end{proof}
\begin{app}
Simplifying matrices.
\end{app}

\begin{thm}[Normal operators over~\(\R\)]
If \(V\)~is finite-dimensional and real and \(T\in\Hom(V)\), then \(T\)~is normal iff \(V\)~has an orthonormal basis with respect to which \(\mat(T)\)~is block diagonal with \(1\)-by-\(1\) blocks and \(2\)-by-\(2\) blocks of the form
\[\left[\begin{matrix}a&-b\\b&a\end{matrix}\right]\qquad(a,b\in\R,b>0)\]
\end{thm}
\begin{proof}[Proof idea]
The reverse direction is immediate by computing matrices.

For the forward direction, proceed by induction on~\(\dim V\). In the induction step, fix a subspace of~\(V\) invariant under~\(T\) of dimension \(1\)~or~\(2\) and use it in an orthogonal decomposition. In case of a \(2\)-dimensional subspace, use the norm characterization of normality to argue that the block of~\(\mat(T)\) corresponding to the subspace has the desired form.
\end{proof}
\begin{app}
Simplifying matrices.
\end{app}

\begin{thm}[Characterizations of positive operators]
If \(V\)~is finite-dimensional and \(T\in\Hom(V)\), the following are equivalent:
\begin{enumerate}[itemsep=0pt]
\item[(a)] \(T\)~is positive
\item[(b)] \(T\)~is self-adjoint and all the eigenvalues of~\(T\) are nonnegative
\item[(c)] \(T\)~has a positive square root
\item[(d)] \(T\)~has a self-adjoint square root
\item[(e)] \(T=S^*S\) for some \(S\in\Hom(V)\)
\end{enumerate}
\end{thm}
\begin{proof}[Proof idea]
For (a)\(\implies\)(b), by direct argument.

For (b)\(\implies\)(c), by the spectral theorem and then taking square roots along the diagonal of~\(\mat(T)\).

For (c)\(\implies\)(d)\(\implies\)(e), by definitions.

For (e)\(\implies\)(a), by direct argument.
\end{proof}

\begin{cor}
Positive operators on finite-dimensional inner product spaces have unique positive square roots.
\end{cor}
\begin{proof}[Proof idea]
To prove uniqueness, note that the eigenvalues of any square root must be precisely the square roots of the eigenvalues of the operator, with corresponding eigenvectors.
\end{proof}

\begin{thm}[Characterizations of isometry in \(\C\)~and~\(\R\)]
Let \(V\)~be finite-dimensional and \(T\in\Hom(V)\).
\begin{enumerate}[itemsep=0pt]
\item[(a)] If \(V\)~is complex, \(T\)~is an isometry iff \(V\)~has an orthonormal basis with respect to which \(\mat(T)\)~is diagonal with \(\abs{\mat(T)_{ii}}=1\) for all~\(i\).
\item[(b)] If \(V\)~is real, \(T\)~is an isometry iff \(V\)~has an orthonormal basis with respect to which \(\mat(T)\)~is block diagonal with blocks either \(1\)-by-\(1\) of the form~\([\lambda]\) with \(\abs{\lambda}=1\) or \(2\)-by-\(2\) of the form
\[\left[\begin{matrix}
\cos\theta&-\sin\theta\\
\sin\theta&\cos\theta
\end{matrix}\right]\]
\end{enumerate}
\end{thm}
\begin{proof}[Proof idea]
For the reverse directions, by computing with matrices. For the forward directions, by the characterizations of normal operators, noting the only possible eigenvalues of an isometry are~\(\pm1\) and, for~(b), that a real isometry of the form
\[\left[\begin{matrix}
a&-b\\
b&a
\end{matrix}\right]\]
with \(b>0\) satisfies \(a^2+b^2=1\), so an appropriate~\(\theta\) exists by polar form of~\((a,b)\).
\end{proof}

\begin{rmk}
An isometry is composed of reflections and rotations.
\end{rmk}

\begin{thm}[Polar decomposition]
If \(V\)~is finite-dimensional and \(T\in\Hom(V)\), there is an isometry \(S\in\Hom(V)\) such that \(T=S\sqrt{T^*T}\).
\end{thm}
\begin{proof}[Proof idea]
By extension. First define \(S:\range\sqrt{T^*T}\to\range T\) by \(\sqrt{T^*T}v\mapsto Tv\), then extend it to an operator on~\(V\) through orthogonal decomposition of~\(V\).
\end{proof}

\begin{thm}[Singular value decomposition]
If \(V\)~is finite-dimensional, \(T\in\Hom(V)\), and \(s_1,\ldots,s_n\) are the singular values of~\(T\), there exist orthonormal bases \((e_1,\ldots,e_n)\) and \((f_1,\ldots,f_n)\) of~\(V\) such that for all \(v\in V\),
\[Tv=s_1\innerprod{v}{e_1}f_1+\cdots+s_n\innerprod{v}{e_n}f_n\]
In particular,
\[\mat(T,(e_1,\ldots,e_n),(f_1,\ldots,f_n))=\left[\begin{matrix}
s_1&&0\\
&\ddots&\\
0&&s_n
\end{matrix}\right]\]
\end{thm}
\begin{proof}[Proof idea]
By polar decomposition and the spectral theorem.

Write \(T=S\sqrt{T^*T}\), \(S\)~an isometry, and let \((e_1,\ldots,e_n)\) be an orthonormal basis of~\(V\) consisting of eigenvectors of~\(\sqrt{T^*T}\) with corresponding eigenvalues \(s_1,\ldots,s_n\). Then for \(v\in V\),
\[Tv=s_1\innerprod{v}{e_1}Se_1+\cdots+s_n\innerprod{v}{e_n}Se_n\]
Now set \(f_i=Se_i\) for \(i\in\{1,\ldots,n\}\).
\end{proof}

\subsection*{Techniques}
\begin{itemize}[itemsep=0pt]
\item Thinking of operators on complex inner product spaces as complex numbers.
\item Using induction on dimension, especially with orthogonal decomposition.
\item Using the norm characterization of normality to analyze matrices.
\item Using the spectral and related theorems to simplify matrices for analysis and computation.
\item Using polar decomposition to break an arbitrary operator into simpler parts.
\item Translating between equality and nullity.
\end{itemize}
